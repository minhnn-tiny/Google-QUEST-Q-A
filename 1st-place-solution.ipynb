{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center> Google Quest Q&A Labeling\n## <center> 1st place solution\n#### <center> by Dmitriy Danevskiy, Oleg Yaroshevskiy, Yury Kashnitsky, and Dmitriy Abulkhanov\n\nThe purpose of this competition is to analyze StackExchange questions & answers predicting whether the question is interesting, whether the answer is helpful or misleading etc. So in theory, top solutions can help Q&A systems in getting more human-like.\n\n\nIn a nutshell, our team trained 4 models: 2 [BERT](https://arxiv.org/abs/1810.04805) ones, one [RoBERTa](https://arxiv.org/abs/1907.11692), and one [BART](https://arxiv.org/abs/1910.13461). Key ideas are:\n- pretraining language models with StackExchange data and auxiliary targets\n- pseudo-labeling\n- postprocessing predictions\n\nDetails are outlined [in this post](https://www.kaggle.com/c/google-quest-challenge/discussion/129840), code is shared in [this repository](https://github.com/oleg-yaroshevskiy/quest_qa_labeling). "},{"metadata":{},"cell_type":"markdown","source":"**Install necessary packages**\n - [mag](https://github.com/ex4sperans/mag) is a lightweight library to keep track of experiments\n - sacremoses is a dependency for transformers\n - sacreBLEU and fairseq are dependencies for the BART model "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n!pip install /kaggle/input/pythonmag/mag > /dev/null\n!pip install ../input/sacremoses/sacremoses-master/ > /dev/null\n!pip install /kaggle/input/sacrebleu/sacreBLEU-master/ > /dev/null\n!pip install /kaggle/input/fairseq-hacked/fairseq > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport json\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Inference\n\n### Model 1. BERT base uncased\n\nThis is an uncased BERT model, its LM is finetuned with StackExchange data."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n!python /kaggle/input/old-bert-code/predict_test.py \\\n  --model_dir /kaggle/input/stackx-80-aux-ep-3       \\\n  --sub_file model1_bert_base_uncased_pred.csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 2. BERT base cased\n\nThis is a cased BERT model, its LM is finetuned with StackExchange data, code has been refactored w.r.t. to the first model."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n!python ../input/bert-base-random-code/run.py                \\\n  --sub_file=model2_bert_base_cased_pred.csv                  \\\n  --data_path=/kaggle/input/google-quest-challenge/            \\\n  --max_sequence_length=500                                     \\\n  --max_title_length=26                                          \\\n  --max_question_length=260                                       \\\n  --max_answer_length=210                                          \\\n  --batch_size=8                                                    \\\n  --bert_model=/kaggle/input/bert-base-pretrained/stackx-base-cased/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 3. RoBERTa\n\nHere we're resorting to both LM finetuning and pseudo-labeling."},{"metadata":{"trusted":true},"cell_type":"code","source":"# setups\n\nROBERTA_EXPERIMENT_DIR = \"2-4-roberta-base-saved-5-head_tail-roberta-stackx-base-v2-pl1kksample20k-1e-05-210-260-500-26-roberta-200\"\n!mkdir $ROBERTA_EXPERIMENT_DIR\n!ln -s /kaggle/input/roberta-stackx-base-pl20k/checkpoints $ROBERTA_EXPERIMENT_DIR/checkpoints\n\nROBERTA_CONFIG = {\n    \"_seed\": 42,\n    \"batch_accumulation\": 2,\n    \"batch_size\": 4,\n    \"bert_model\": \"roberta-base-saved\",\n    \"folds\": 5,\n    \"head_tail\": True,\n    \"label\": \"roberta-stackx-base-v2-pl1kksample20k\",\n    \"lr\": 1e-05,\n    \"max_answer_length\": 210,\n    \"max_question_length\": 260,\n    \"max_sequence_length\": 500,\n    \"max_title_length\": 26,\n    \"model_type\": \"roberta\",\n    \"warmup\": 200\n}\nwith open(os.path.join(ROBERTA_EXPERIMENT_DIR, \"config.json\"), \"w\") as fp:\n    json.dump(ROBERTA_CONFIG, fp)\n    \n!echo kek > $ROBERTA_EXPERIMENT_DIR/command","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n!python ../input/roberta-base-code/infer.py                 \\\n  --experiment $ROBERTA_EXPERIMENT_DIR                       \\\n  --checkpoint=best_model.pth                                 \\\n  --bert_model=/kaggle/input/roberta-base-model                \\\n  --dataframe=/kaggle/input/google-quest-challenge/test.csv     \\\n  --output_dir=roberta-base-output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Model 4. BART\n\nBART-large, with pseudo-labeling."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\n!python ../input/bart-code/run.py                      \\\n  --sub_file=model4_bart_large_pred.csv                 \\\n  --data_path=/kaggle/input/google-quest-challenge/      \\\n  --max_sequence_length=500                               \\\n  --max_title_length=26                                    \\\n  --max_question_length=260                                 \\\n  --max_answer_length=210                                    \\\n  --batch_size=4                                              \\\n  --bert_model=bart.large","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Blending and postprocessing"},{"metadata":{},"cell_type":"markdown","source":"**First, we read the 30 target columns that we need to predict.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_df = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\", \n                             index_col='qa_id')\ntarget_columns = sample_submission_df.columns\nprint(f'There are {len(target_columns)} targets to predict')\n\ntrain_df = pd.read_csv(\"/kaggle/input/google-quest-challenge/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading submission files**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1_bert_base_uncased_pred_df = pd.read_csv(\"model1_bert_base_uncased_pred.csv\")\nmodel2_bert_base_cased_pred_df = pd.read_csv(\"model2_bert_base_cased_pred.csv\")\nmodel4_bart_large_pred_df = pd.read_csv(\"model4_bart_large_pred.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For RoBERTa, we average predictions from 5 folds**"},{"metadata":{"trusted":true},"cell_type":"code","source":"roberta_base_dfs = [pd.read_csv(\n                    os.path.join(\"roberta-base-output\", \"fold-{}.csv\".format(fold))) \n                    for fold in range(5)]\n\nmodel3_roberta_pred_df = roberta_base_dfs[0].copy()\n\nfor col in target_columns:\n    model3_roberta_pred_df[col] = np.mean([df[col] for df in roberta_base_dfs], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**Blending**"},{"metadata":{"trusted":true},"cell_type":"code","source":"blended_df = model3_roberta_pred_df.copy()\n\nfor col in target_columns:\n    blended_df[col] = (\n        model1_bert_base_uncased_pred_df[col] * 0.1 +\n        model2_bert_base_cased_pred_df[col] * 0.2 + \n        model3_roberta_pred_df[col] * 0.1 + \n        model4_bart_large_pred_df[col] * 0.3\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Applying postprocessing to the final blend, also discussed [here](https://www.kaggle.com/c/google-quest-challenge/discussion/129840).**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def postprocess_single(target, ref):\n    \"\"\"\n    The idea here is to make the distribution of a particular predicted column\n    to match the correspoding distribution of the corresponding column in the\n    training dataset (called ref here)\n    \"\"\"\n    \n    ids = np.argsort(target)\n    counts = sorted(Counter(ref).items(), key=lambda s: s[0])\n    scores = np.zeros_like(target)\n    \n    last_pos = 0\n    v = 0\n    \n    for value, count in counts:\n        next_pos = last_pos + int(round(count / len(ref) * len(target)))\n        if next_pos == last_pos:\n            next_pos += 1\n\n        cond = ids[last_pos:next_pos]\n        scores[cond] = v\n        last_pos = next_pos\n        v += 1\n        \n    return scores / scores.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def postprocess_prediction(prediction, actual):\n    \n    postprocessed = prediction.copy()\n    \n    for col in target_columns:\n        scores = postprocess_single(prediction[col].values, actual[col].values)\n        # Those are columns where our postprocessing gave substantial improvement.\n        # It also helped for some others, but we didn't include them as the gain was\n        # very marginal (less than 0.01)\n        if col in (\n            \"question_conversational\",\n            \"question_type_compare\",\n            \"question_type_definition\",\n            \"question_type_entity\",\n            \"question_has_commonly_accepted_answer\",\n            \"question_type_consequence\",\n            \"question_type_spelling\"\n        ):\n            postprocessed[col] = scores\n            \n        # scale to 0-1 interval\n        v = postprocessed[col].values\n        postprocessed[col] = (v - v.min()) / (v.max() - v.min())\n    \n    return postprocessed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"postprocessed = postprocess_prediction(blended_df, train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Saving the submission file.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"postprocessed.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}